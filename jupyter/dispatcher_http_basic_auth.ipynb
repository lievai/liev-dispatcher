{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1f3b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import io\n",
    "import sys\n",
    "from PIL import Image\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import webbrowser\n",
    "import time\n",
    "\n",
    "username = 'lievuser'\n",
    "password = 'changeme'\n",
    "server = 'http://localhost:5011/'\n",
    "auth = HTTPBasicAuth(username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f367e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(instruction, function=None, system_msg=\"\", max_new_tokens=18000, temperature=0.1, timeout=120, llm_name=None, try_next_on_failure = True):\n",
    "    data = {\n",
    "       \"function\": function,\n",
    "       \"instruction\": instruction,\n",
    "       \"system_msg\": system_msg,\n",
    "       \"max_new_tokens\": max_new_tokens,\n",
    "       \"temperature\": temperature,\n",
    "       \"timeout\": timeout,\n",
    "       \"llm_name\": llm_name,\n",
    "       \"try_next_on_failure\": try_next_on_failure\n",
    "    }\n",
    "    load = requests.get(server+\"response\", data = json.dumps(data), auth=auth)\n",
    "    if load.status_code == 200:\n",
    "        print(f'\\033[92m Response Headers\\n Liev-Response-Model: {load.headers[\"Liev-Response-Model\"]}\\033[31m\\n Liev-Response-Is-Failover: {load.headers[\"Liev-Response-Is-Failover\"]}\\n Liev-Response-Failed-Models: {load.headers[\"Liev-Response-Failed-Models\"]}\\033[0m')\n",
    "        if \"image\" in load.headers['content-type']:\n",
    "            return Image.open(io.BytesIO(load.content))\n",
    "        else:\n",
    "            return json.loads(load.text)\n",
    "    else:\n",
    "        sys.exit(\"Error calling the orchestrator. Erro code: \" + str(load.status_code) + str(load.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7420de",
   "metadata": {},
   "source": [
    "### Listar todas as LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5967edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available LLMs\n",
    "# It can be retrieved by GET in the endpoint /v1/llm\n",
    "\n",
    "json_llms = json.loads(requests.get(server+\"v1/llms_and_types\", auth=auth).text)\n",
    "\n",
    "df = pd.DataFrame(json_llms)\n",
    "selected_columns = ['type', 'name', 'model',  'priority', 'response_mime', 'is_external']\n",
    "df_selected = df[selected_columns]\n",
    "df_selected_sorted = df_selected.sort_values(by=['type', 'priority'])\n",
    "df_selected_sorted = df_selected_sorted.reset_index(drop=True)\n",
    "print (server+\"v1/llms_and_types\")\n",
    "df_selected_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcd36da",
   "metadata": {},
   "source": [
    "### Listar LLMs por tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a52174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available LLMs By Type\n",
    "# It can be retrieved by GET in the endpoint /v1/llm/<type>\n",
    "\n",
    "json_llms = json.loads(requests.get(server+\"v1/llms_and_types/code\", headers={'Authorization': f\"Bearer {access_token}\"}).text)\n",
    "\n",
    "df = pd.DataFrame(json_llms)\n",
    "selected_columns = ['type', 'name', 'model',  'priority', 'response_mime']\n",
    "df_selected = df[selected_columns]\n",
    "df_selected_sorted = df_selected.sort_values(by=['type', 'priority'])\n",
    "df_selected_sorted = df_selected_sorted.reset_index(drop=True)\n",
    "print (server+\"v1/llm/code\")\n",
    "df_selected_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614fbb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt detection used by orchestrator\n",
    "text = \"\"\"\n",
    "Classify the following prompt between the categories. Return just the category name. Prompt: Código C++ para if. Allowed categories: text,image,embeddings,code,audio Category:\"\"\"\n",
    "system_msg=\"You are a content classifier\"\n",
    "print(ask(text+\"\\n.\\n\", system_msg=system_msg, function=\"text\",llm_name=\"codellama\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b19c19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misterious LLM with no type informed!\n",
    "# The following request doesn't use the parameter \"function\". The orchestrator classifies the prompt and returns according.\n",
    "\n",
    "\n",
    "print('\\033[93m Asking for a story \\033[0m')\n",
    "print('\\033[93m----------------------------------------------------\\033[0m')\n",
    "\n",
    "response = ask(\"Conte-me a história do Desastre de messênia\")\n",
    "print(response)\n",
    "\n",
    "print('\\033[93m Asking for a code \\033[0m')\n",
    "print('\\033[93m----------------------------------------------------\\033[0m')\n",
    "\n",
    "response = ask(\"Gimme a sample C++ REST API\")\n",
    "print(response)\n",
    "\n",
    "print('\\033[93m Asking for something like an image \\033[0m')\n",
    "print('\\033[93m----------------------------------------------------\\033[0m')\n",
    "response = ask(\"A photo of a delicious pizza of burrata and brie cheese\")\n",
    "display(response)\n",
    "\n",
    "print('\\033[93m Offending the LLM! \\033[0m')\n",
    "print('\\033[93m----------------------------------------------------\\033[0m')\n",
    "response = ask(\"You're so ugly. What a shame!\")\n",
    "display(response)\n",
    "\n",
    "\n",
    "print('\\033[93m Asking for embbed a content! \\033[0m')\n",
    "print('\\033[93m----------------------------------------------------\\033[0m')\n",
    "response = ask(\"Embbed the following sentence: 'I'm asking for an image, a text or a code?'\")\n",
    "display(response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f101f7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text request , LLM autoselected by priority\n",
    "text = \"\"\"Título: O desastre do barco de migrantes na Messênia.\"\"\"\n",
    "system_msg=\"Você é um escritor brasileiro.\"\n",
    "print(ask(text+\"\\nDado o título acima de um artigo imaginário, escreva um artigo com 200 palavras.\\n\", system_msg=system_msg, function='text'))\n",
    "\n",
    "\n",
    "#Text request , with LLM specified by the llm_name parameter\n",
    "text = \"\"\"Título: O desastre do barco de migrantes na Messênia.\"\"\"\n",
    "system_msg=\"Você é um escritor brasileiro.\"\n",
    "print(ask(text+\"\\nDado o título acima de um artigo imaginário, escreva um artigo com 200 palavras.\\n\", system_msg=system_msg, function='text', llm_name=\"claude\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017053be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image request , LLM autoselected by priority\n",
    "instruction = \"\"\"A dog with a weird hat.\"\"\"\n",
    "image = ask(instruction, function=\"image\")\n",
    "display(image)\n",
    "#Image request , with LLM specified by the llm_name parameter\n",
    "instruction = \"\"\"A cat playing with a ball.\"\"\"\n",
    "image = ask(instruction, function=\"image\", llm_name=\"stablediffusion\")\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30536090-b3d9-4b8e-a806-7364834bc02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Code request , LLM autoselected by priority\n",
    "# prompt = \"\"\"Escreva um programa em python que imprima todos os números divisíveis por 3 e 7 entre 1 e 100.\"\"\"\n",
    "# print(ask(prompt, function=\"code\"))\n",
    "\n",
    "#Code request , with LLM specified by the llm_name parameter\n",
    "prompt = \"\"\"Escreva um programa em Cobol que imprima todos os números divisíveis por 3 e 7 entre 1 e 100.\"\"\"\n",
    "print(ask(prompt, function=\"code\", llm_name=\"openai\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad69143",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asking for type and text, but the llms doesn't work. It gets failed over\n",
    "prompt = \"\"\"Como surgiram os aborígenes?\"\"\"\n",
    "print(ask(prompt, function=\"text\", llm_name=\"mixtral\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acce201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asking for type and text, the llm doesn't work and I don't want failover. It needs to fail if specific LLM is not available\n",
    "# IT MUST FAIL\n",
    "prompt = \"\"\"Como surgiram os aborígenes?\"\"\"\n",
    "print(ask(prompt, function=\"text\", llm_name = \"mixtral\", try_next_on_failure=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0574f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the LLM just by it's name.\n",
    "#Sometimes the Same LLM can work with multiple types(functions), like text or code\n",
    "#Some get better responses than others depending of type. That's why LLM failover priorities is made based on type.\n",
    "#Because type is not passed and detection is not being made - since i'm telling what llm is - this mode DOESN'T SUPPORT failover\n",
    "#Usually, who calls the llm directly knows what is doing\n",
    "prompt = \"\"\"Crie um plano de teste, com alguns casos de teste, para um jogo da velha\"\"\"\n",
    "print(ask(prompt, llm_name = \"vicuna\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03823f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi LLM Text Request\n",
    "prompt = \"\"\"Which is the planet with most moons?\"\"\"\n",
    "print(json.dumps(ask(prompt, function=\"text\", llm_name=\"all\"), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8385e458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi LLM Embeddings Request\n",
    "prompt = \"\"\"\n",
    "Contrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of \"de Finibus Bonorum et Malorum\" (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, \"Lorem ipsum dolor sit amet..\", comes from a line in section 1.10.32.\n",
    "\"\"\"\n",
    "print(json.dumps(ask(prompt, function=\"embeddings\", llm_name=\"all\"), indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
